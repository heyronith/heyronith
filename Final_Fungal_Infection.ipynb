{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7B8nw7i1T7U32V0VMs4EP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heyronith/heyronith/blob/main/Final_Fungal_Infection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSMrAoOUtfjW",
        "outputId": "3d8ea269-e8fb-493b-b8c1-b27c3816826f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Found 1680 images belonging to 4 classes.\n",
            "Found 417 images belonging to 4 classes.\n",
            "Epoch 1/25\n",
            "53/53 [==============================] - 10s 161ms/step - loss: 1.1403 - accuracy: 0.4821 - val_loss: 0.8147 - val_accuracy: 0.6499\n",
            "Epoch 2/25\n",
            "53/53 [==============================] - 7s 138ms/step - loss: 0.7679 - accuracy: 0.6381 - val_loss: 0.6893 - val_accuracy: 0.6715\n",
            "Epoch 3/25\n",
            "53/53 [==============================] - 8s 148ms/step - loss: 0.6780 - accuracy: 0.6893 - val_loss: 0.6354 - val_accuracy: 0.6882\n",
            "Epoch 4/25\n",
            "53/53 [==============================] - 7s 131ms/step - loss: 0.6598 - accuracy: 0.6851 - val_loss: 0.6496 - val_accuracy: 0.7026\n",
            "Epoch 5/25\n",
            "53/53 [==============================] - 8s 155ms/step - loss: 0.6065 - accuracy: 0.7238 - val_loss: 0.6450 - val_accuracy: 0.6715\n",
            "Epoch 6/25\n",
            "53/53 [==============================] - 8s 142ms/step - loss: 0.6284 - accuracy: 0.7083 - val_loss: 0.6034 - val_accuracy: 0.7458\n",
            "Epoch 7/25\n",
            "53/53 [==============================] - 8s 145ms/step - loss: 0.6271 - accuracy: 0.6917 - val_loss: 0.6412 - val_accuracy: 0.7314\n",
            "Epoch 8/25\n",
            "53/53 [==============================] - 8s 150ms/step - loss: 0.6028 - accuracy: 0.7220 - val_loss: 0.5959 - val_accuracy: 0.7290\n",
            "Epoch 9/25\n",
            "53/53 [==============================] - 7s 132ms/step - loss: 0.5836 - accuracy: 0.7185 - val_loss: 0.5468 - val_accuracy: 0.7434\n",
            "Epoch 10/25\n",
            "53/53 [==============================] - 7s 139ms/step - loss: 0.5935 - accuracy: 0.7196 - val_loss: 0.5481 - val_accuracy: 0.7482\n",
            "Epoch 11/25\n",
            "53/53 [==============================] - 9s 164ms/step - loss: 0.5719 - accuracy: 0.7185 - val_loss: 0.5687 - val_accuracy: 0.7482\n",
            "Epoch 12/25\n",
            "53/53 [==============================] - 7s 132ms/step - loss: 0.5953 - accuracy: 0.7280 - val_loss: 0.5524 - val_accuracy: 0.7338\n",
            "Epoch 13/25\n",
            "53/53 [==============================] - 7s 133ms/step - loss: 0.5865 - accuracy: 0.7357 - val_loss: 0.5356 - val_accuracy: 0.7578\n",
            "Epoch 14/25\n",
            "53/53 [==============================] - 8s 151ms/step - loss: 0.5590 - accuracy: 0.7548 - val_loss: 0.5090 - val_accuracy: 0.7650\n",
            "Epoch 15/25\n",
            "53/53 [==============================] - 7s 135ms/step - loss: 0.5569 - accuracy: 0.7417 - val_loss: 0.5404 - val_accuracy: 0.7722\n",
            "Epoch 16/25\n",
            "53/53 [==============================] - 8s 144ms/step - loss: 0.5872 - accuracy: 0.7101 - val_loss: 0.5160 - val_accuracy: 0.7314\n",
            "Epoch 17/25\n",
            "53/53 [==============================] - 8s 154ms/step - loss: 0.5422 - accuracy: 0.7494 - val_loss: 0.5416 - val_accuracy: 0.7602\n",
            "Epoch 18/25\n",
            "53/53 [==============================] - 7s 132ms/step - loss: 0.5281 - accuracy: 0.7613 - val_loss: 0.4973 - val_accuracy: 0.7986\n",
            "Epoch 19/25\n",
            "53/53 [==============================] - 8s 150ms/step - loss: 0.5190 - accuracy: 0.7696 - val_loss: 0.5215 - val_accuracy: 0.7650\n",
            "Epoch 20/25\n",
            "53/53 [==============================] - 8s 159ms/step - loss: 0.4895 - accuracy: 0.7804 - val_loss: 0.4637 - val_accuracy: 0.8129\n",
            "Epoch 21/25\n",
            "53/53 [==============================] - 7s 133ms/step - loss: 0.5036 - accuracy: 0.7756 - val_loss: 0.4465 - val_accuracy: 0.8058\n",
            "Epoch 22/25\n",
            "53/53 [==============================] - 8s 151ms/step - loss: 0.5232 - accuracy: 0.7625 - val_loss: 0.4909 - val_accuracy: 0.7914\n",
            "Epoch 23/25\n",
            "53/53 [==============================] - 7s 131ms/step - loss: 0.5157 - accuracy: 0.7702 - val_loss: 0.4700 - val_accuracy: 0.8106\n",
            "Epoch 24/25\n",
            "53/53 [==============================] - 7s 140ms/step - loss: 0.5000 - accuracy: 0.7798 - val_loss: 0.4504 - val_accuracy: 0.8129\n",
            "Epoch 25/25\n",
            "53/53 [==============================] - 8s 148ms/step - loss: 0.5115 - accuracy: 0.7655 - val_loss: 0.5747 - val_accuracy: 0.7626\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import os\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Define the parameters\n",
        "IMAGE_SIZE = (64, 64)\n",
        "BATCH_SIZE = 32\n",
        "DIRECTORY = '/content/drive/MyDrive/C'\n",
        "\n",
        "# Apply data augmentation\n",
        "datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
        "                             rotation_range=20,\n",
        "                             zoom_range=0.15,\n",
        "                             width_shift_range=0.2,\n",
        "                             height_shift_range=0.2,\n",
        "                             shear_range=0.15,\n",
        "                             horizontal_flip=True,\n",
        "                             fill_mode=\"nearest\",\n",
        "                             validation_split=0.2)\n",
        "\n",
        "# Prepare the iterators\n",
        "train_it = datagen.flow_from_directory(DIRECTORY,\n",
        "                                       target_size=IMAGE_SIZE,\n",
        "                                       batch_size=BATCH_SIZE,\n",
        "                                       class_mode='categorical',\n",
        "                                       color_mode='grayscale',  # Set color_mode to 'grayscale'\n",
        "                                       subset='training')\n",
        "val_it = datagen.flow_from_directory(DIRECTORY,\n",
        "                                     target_size=IMAGE_SIZE,\n",
        "                                     batch_size=BATCH_SIZE,\n",
        "                                     class_mode='categorical',\n",
        "                                     color_mode='grayscale',  # Set color_mode to 'grayscale'\n",
        "                                     subset='validation')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=IMAGE_SIZE + (1,)))  # Change the number of channels to 1\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(train_it.num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "\n",
        "history = model.fit(train_it,\n",
        "                    steps_per_epoch=len(train_it),\n",
        "                    validation_data=val_it,\n",
        "                    validation_steps=len(val_it),\n",
        "                    epochs=25)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model_cnn.h5')"
      ],
      "metadata": {
        "id": "ow7_p4GHZc3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(val_it, steps=len(val_it))\n",
        "print(\"Validation accuracy: {:.2f}%\".format(accuracy * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aexrLBTGuSUN",
        "outputId": "c72e8bd2-dc78-485f-9090-b67ab2a4391d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 2s 103ms/step - loss: 0.5659 - accuracy: 0.7386\n",
            "Validation accuracy: 73.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import img_to_array  # Correct import here\n",
        "\n",
        "def load_image(img_path, size):\n",
        "    # Load the image\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Convert the image to 3 channels\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    # Resize the image\n",
        "    img = cv2.resize(img, size)\n",
        "\n",
        "    # Convert the image data to a 4D tensor\n",
        "    img_tensor = img_to_array(img)  # Correct usage here\n",
        "\n",
        "    # The model expects input in this form ((1, size, size, 3))\n",
        "    img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "\n",
        "    # Follow the Original Model preprocessing\n",
        "    img_tensor /= 255.\n",
        "\n",
        "    return img_tensor\n",
        "\n",
        "img_path = \"/content/OIP.jpg\"\n",
        "\n",
        "# The original model was trained on 64x64 images\n",
        "img = load_image(img_path, size=(64,64))\n",
        "\n",
        "\n",
        "# Get the index of the highest probability\n",
        "class_index = np.argmax(history)\n",
        "\n",
        "# Now get the class label\n",
        "class_label = train_it.class_indices\n",
        "class_label = {v: k for k, v in class_label.items()}\n",
        "predicted_class = class_label[class_index]\n",
        "\n",
        "print(predicted_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-wYwkuMz5AQ",
        "outputId": "dc37b7d2-ce3e-4947-8415-0bc5134cc2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VpZ_71eJa-EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jhxu8Rb4a-G7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}